# Evaluations-Standard

## Hinweise
*	gleiche Stichproben zur Vergleichbarkeit
*	Datenbank aus „einfachen Sätzen“ (siehe Praktikums-Rechner)
*	IOB chunk representation (einheitliches Format)
*	Zu vergleichende Tags: 
**	PER (Person)
**	LOC (Location)
**	ORG (Organisation)

## Allgemein (Arbeit mit Datenbank)
*	wie viele Tags werden von den einzelnen Tools insgesamt gefunden? 
*	Wie viele der jeweiligen Tags werden von den einzelnen Tools gefunden? 
*	Stichprobenartig: Stimmen die gefundenen Tags überein? 
*	Stichprobenartig: Welche Wörter, die einem Tag zugeordnet werden sollten, werden gar nicht entdeckt?

## Spezifische Fragestellungen (eigene Datenbank)
*	Verhalten des Tools bei Entities mit mehreren Klassen (z.B. Name und Ort gleichzeitig)
*	Unterschiedliches Verhalten der Tools bei Groß- und Kleinschreibung?
*	Verhalten bei Wörtern, die Initialen beinhalten?  
*	Verhalten bei Akronymen (Kurzwörter aus Anfangsbuchstaben, z.B. ASAP)?
*	Verhalten bei Ausdrücken mit Präpositionen (z.B. Institut für Medienpädagogik)?

## Evaluierungsmaße
*	Precision = Anzahl korrekt klassifizierter Nes / Anzahl NEs gefunden
*	Recall = Anzahl korrekt klassifizierter NEs / Anzahl vorhandener NEs
*	F-Test = (2 * Precision * Recall) / (Precision + Recall)
